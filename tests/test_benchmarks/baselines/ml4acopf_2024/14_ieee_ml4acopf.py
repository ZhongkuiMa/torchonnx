"""PyTorch module generated by TorchONNX.

Source: /mnt/hdd1/zhongkui/rover_project/rover_alpha/torchonnx/tests/vnncomp2024_benchmarks/ml4acopf_2024/onnx/14_ieee_ml4acopf.onnx
Generated: 2025-12-22 03:36:18

This module was automatically converted from an ONNX model.
"""

__all__ = ["Ml4acopf202414IeeeMl4acopf"]

import torch
import torch.nn as nn


def dynamic_expand(data, target_shape):
    """Vmap-compatible dynamic expand helper for ONNX Expand operation.

    This version handles the conversion from ONNX semantics to PyTorch
    while remaining compatible with vmap.

    Note: For full vmap compatibility, target_shape should be constant
    (known at code generation time). Dynamic target_shape from tensors
    may still work but with limitations.
    """
    # Convert target_shape to list of ints
    if isinstance(target_shape, torch.Tensor):
        target_shape = target_shape.to(torch.int64).tolist()
    target_shape = [int(x) for x in target_shape]

    # If data has more dimensions than target, squeeze leading dimensions
    if data.ndim > len(target_shape):
        new_shape = tuple(int(s) for s in data.shape[data.ndim - len(target_shape) :])
        data = data.reshape(new_shape)

    # Convert ONNX semantics to PyTorch
    # ONNX: 1 means "keep dimension", PyTorch: -1 means "keep dimension"
    converted_shape = []
    offset = len(target_shape) - data.ndim

    for i in range(len(target_shape)):
        if i < offset:
            # Dimension doesn't exist in data, use target value
            converted_shape.append(target_shape[i])
        else:
            # Dimension exists in data
            data_idx = i - offset
            if target_shape[i] == 1 and data.shape[data_idx] != 1:
                # Keep data's dimension
                converted_shape.append(-1)
            else:
                # Use target dimension
                converted_shape.append(target_shape[i])

    return data.expand(*converted_shape)


class Ml4acopf202414IeeeMl4acopf(nn.Module):
    def __init__(self):
        super().__init__()
        self.register_buffer("c4", torch.empty([24], dtype=torch.float32))
        self.register_buffer("c5", torch.empty([24], dtype=torch.float32))
        self.register_buffer("c34", torch.empty([20], dtype=torch.int64))
        self.register_buffer("c35", torch.empty([20], dtype=torch.int64))
        self.register_buffer("c38", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c39", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c40", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c41", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c42", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c43", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c44", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c45", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c46", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c47", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c50", torch.empty([2], dtype=torch.int64))
        self.register_buffer("c51", torch.empty([20], dtype=torch.float32))
        self.register_buffer("c54", torch.empty([2], dtype=torch.int64))
        self.register_buffer("c59", torch.empty([14, 11], dtype=torch.float32))
        self.register_buffer("c60", torch.empty([14, 5], dtype=torch.float32))
        self.register_buffer("c61", torch.empty([14, 20], dtype=torch.float32))
        self.register_buffer("c62", torch.empty([14, 20], dtype=torch.float32))
        self.register_buffer("c64", torch.empty([14], dtype=torch.float32))
        self.register_buffer("c65", torch.empty([14], dtype=torch.float32))
        self.linear1 = nn.Linear(22, 32)
        self.relu1 = nn.ReLU()
        self.linear2 = nn.Linear(32, 32)
        self.relu2 = nn.ReLU()
        self.linear3 = nn.Linear(32, 32)
        self.relu3 = nn.ReLU()
        self.linear4 = nn.Linear(32, 38)
        self.sigmoid1 = nn.Sigmoid()

    def forward(self, x0):
        x1 = self.linear1(x0)
        x2 = self.relu1(x1)
        x3 = self.linear2(x2)
        x4 = self.relu2(x3)
        x5 = self.linear3(x4)
        x6 = self.relu3(x5)
        x7 = self.linear4(x6)
        x8 = x7[:, 0:24]
        x9 = self.sigmoid1(x8)
        x10 = x9 * self.c4
        x11 = x10 + self.c5
        x12 = x7[:, 24:]
        x13 = torch.cat([x11, x12], dim=1)
        x14 = x0[:, 0:11]
        x15 = x0[:, 11:22]
        x16 = x13[:, 0:5]
        x17 = x13[:, 5:10]
        x18 = x13[:, 10:24]
        x19 = x13[:, 24:38]
        x20 = x19.index_select(1, self.c34.reshape(-1).long())
        x21 = x19.index_select(1, self.c35.reshape(-1).long())
        x22 = x20 - x21
        x23 = x18.index_select(1, self.c34.reshape(-1).long())
        x24 = x18.index_select(1, self.c35.reshape(-1).long())
        x25 = x23**2
        x26 = x24**2
        x27 = x22.cos()
        x28 = x22.sin()
        x29 = x23 * x24
        x30 = x29 * x27
        x31 = x29 * x28
        x32 = -x31
        x33 = self.c38 * x25
        x34 = self.c39 * x30
        x35 = x33 + x34
        x36 = self.c40 * x31
        x37 = x35 + x36
        x38 = self.c41 * x25
        x39 = self.c42 * x30
        x40 = x38 - x39
        x41 = self.c43 * x31
        x42 = x40 + x41
        x43 = self.c44 * x26
        x44 = x43 + x34
        x45 = self.c45 * x32
        x46 = x44 + x45
        x47 = self.c46 * x26
        x48 = x47 - x39
        x49 = self.c47 * x32
        x50 = x48 + x49
        x51 = torch.full((2,), 1.0, dtype=torch.int64, device=x0.device)
        x52 = x51 * -1
        x53 = self.c50 == x52
        x54 = torch.where(x53, x51, self.c50)
        x55 = dynamic_expand(self.c51, x54)
        x56 = torch.full((2,), 1.0, dtype=torch.int64, device=x0.device)
        x57 = x56 * -1
        x58 = self.c54 == x57
        x59 = torch.where(x58, x56, self.c54)
        x60 = dynamic_expand(self.c51, x59)
        x61 = x37**2
        x62 = x42**2
        x63 = x61 + x62
        x64 = x63 - x55
        x65 = x46**2
        x66 = x50**2
        x67 = x65 + x66
        x68 = x67 - x60
        x69 = x14.permute((1, 0))
        x70 = self.c59 @ x69
        x71 = x70.permute((1, 0))
        x72 = x15.permute((1, 0))
        x73 = self.c59 @ x72
        x74 = x73.permute((1, 0))
        x75 = x16.permute((1, 0))
        x76 = self.c60 @ x75
        x77 = x76.permute((1, 0))
        x78 = x17.permute((1, 0))
        x79 = self.c60 @ x78
        x80 = x79.permute((1, 0))
        x81 = x37.permute((1, 0))
        x82 = self.c61 @ x81
        x83 = x82.permute((1, 0))
        x84 = x46.permute((1, 0))
        x85 = self.c62 @ x84
        x86 = x85.permute((1, 0))
        x87 = x42.permute((1, 0))
        x88 = self.c61 @ x87
        x89 = x88.permute((1, 0))
        x90 = x50.permute((1, 0))
        x91 = self.c62 @ x90
        x92 = x91.permute((1, 0))
        x93 = x18**2
        x94 = x77 - x71
        x95 = x94 - x86
        x96 = x95 - x83
        x97 = self.c64 * x93
        x98 = x96 - x97
        x99 = x80 - x74
        x100 = x99 - x92
        x101 = x100 - x89
        x102 = self.c65 * x93
        x103 = x101 + x102
        x104 = torch.cat([x13, x37, x46, x42, x50, x64, x68, x98, x103], dim=1)
        return x104
